library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
splitOn <- cut2(training$Age, g = 4)
splitOn <- mapvalues(splitOn,
from = levels(factor(splitOn)),
to = c("red", "blue", "yellow", "green"))
# automatically includes index of samples
plot(training$CompressiveStrength, col = splitOn)
library(caret)
library(Hmisc)
install.packages("Hmisc")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
library(Hmisc)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
splitOn <- cut2(training$Age, g = 4)
splitOn <- mapvalues(splitOn,
from = levels(factor(splitOn)),
to = c("red", "blue", "yellow", "green"))
# automatically includes index of samples
plot(training$CompressiveStrength, col = splitOn)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_variables <- grep("^IL", names(training), value = TRUE)
preProc <- preProcess(training[, IL_variables], method = "pca", thresh = 0.9)
preProc
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
install.packages("e1071")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
model1
model2
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(training)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
?train
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_variables <- grep("^IL", names(training), value = TRUE)
preProc <- preProcess(training[, IL_variables], method = "pca", thresh = 0.8)
preProc
library(devtools)
install_github(slidify,ramnathv)
install_github('slidify','ramnathv')
install_github('slidifyLibrairies','ramnathv')
install_github('slidifyLibrairies','ramnathv:slidifyLibrairies')
install_github('slidifyLibrairies/slidifyLibrairies','ramnathv:slidifyLibrairies')
install_github('slidifyLibrairies/slidifyLibrairies','ramnathv/slidifyLibrairies')
install_github('slidifyLibrairies/slidifyLibrairies','ramnathv/slidifyLibrairies')
install_github('slidifyLibrairies/slidifyLibrairies','ramnathv')
install_github('slidifyLibraries','ramnathv')
install.packages("plyr")
xTrain <- read.table("./UCI HAR Dataset/train/X_train.txt")
xTest <- read.table("./UCI HAR Dataset/test/X_test.txt")
yTrain <- read.table("./UCI HAR Dataset/train/y_train.txt")
yTest <- read.table("./UCI HAR Dataset/test/y_test.txt")
subjectTrain <- read.table("./UCI HAR Dataset/train/subject_train.txt")
subjectTest <- read.table("./UCI HAR Dataset/test/subject_test.txt")
activityLabels <- read.table("./UCI HAR Dataset/activity_labels.txt")
features <- read.table("./UCI HAR Dataset/features.txt")
# Merge test and train data
subject <- rbind(subjectTest, subjectTrain)
colnames(subject) <- "subject"
y <- rbind(yTest, yTrain)
y <- merge(y, activityLabels, by=1)[,2]
x <- rbind(xTest, xTrain)
colnames(x) <- features[, 2]
x <- cbind(subject, y, x)
setwd("~/GitHub/getting-and-cleaning-data-course-project")
xTrain <- read.table("./UCI HAR Dataset/train/X_train.txt")
xTest <- read.table("./UCI HAR Dataset/test/X_test.txt")
yTrain <- read.table("./UCI HAR Dataset/train/y_train.txt")
yTest <- read.table("./UCI HAR Dataset/test/y_test.txt")
subjectTrain <- read.table("./UCI HAR Dataset/train/subject_train.txt")
subjectTest <- read.table("./UCI HAR Dataset/test/subject_test.txt")
activityLabels <- read.table("./UCI HAR Dataset/activity_labels.txt")
features <- read.table("./UCI HAR Dataset/features.txt")
# Merge test and train data
subject <- rbind(subjectTest, subjectTrain)
colnames(subject) <- "subject"
y <- rbind(yTest, yTrain)
y <- merge(y, activityLabels, by=1)[,2]
x <- rbind(xTest, xTrain)
colnames(x) <- features[, 2]
x <- cbind(subject, y, x)
names(x,y,subject)
names(x)
features
yTest$
s
yTest
names(yTest)
y
names(y)
features[1,]
x[1,]
?grep
search <- grep("-mean|-std", colnames(x))
data.mean.std <- x[,c(1,2,search)]
melted = melt(data.mean.std, id.var = c("Subject", "Activity"))
means = dcast(melted , subject + y ~ variable, mean)
library(reshape2)
melted = melt(data.mean.std, id.var = c("Subject", "Activity"))
means = dcast(melted , subject + y ~ variable, mean)
xTrain <- read.table("./UCI HAR Dataset/train/X_train.txt")
xTest <- read.table("./UCI HAR Dataset/test/X_test.txt")
yTrain <- read.table("./UCI HAR Dataset/train/y_train.txt")
yTest <- read.table("./UCI HAR Dataset/test/y_test.txt")
subjectTrain <- read.table("./UCI HAR Dataset/train/subject_train.txt")
subjectTest <- read.table("./UCI HAR Dataset/test/subject_test.txt")
activityLabels <- read.table("./UCI HAR Dataset/activity_labels.txt")
features <- read.table("./UCI HAR Dataset/features.txt")
# Merge test and train data
subject <- rbind(subjectTest, subjectTrain)
colnames(subject) <- "Subject"
y <- rbind(yTest, yTrain)
y <- merge(y, activityLabels, by = 1)[, 2]
colnames(y) <- "Activity"
x <- rbind(xTest, xTrain)
colnames(x) <- features[, 2]
x <- cbind(subject, y, x)
# Mean and STD
search <- grep("-mean|-std", colnames(x))
data.mean.std <- x[,c(1,2,search)]
library(reshape2)
melted = melt(data.mean.std, id.var = c("Subject", "Activity"))
means = dcast(melted , subject + y ~ variable, mean)
y <- rbind(yTest, yTrain)
colnames(y) <- "Activity"
y <- merge(y, activityLabels, by = 1)[, 2]
x <- rbind(xTest, xTrain)
colnames(x) <- features[, 2]
x <- cbind(subject, y, x)
# Mean and STD
search <- grep("-mean|-std", colnames(x))
data.mean.std <- x[,c(1,2,search)]
library(reshape2)
melted = melt(data.mean.std, id.var = c("Subject", "Activity"))
means = dcast(melted , subject + y ~ variable, mean)
Activity <- rbind(yTest, yTrain)
Activity <- merge(Activity, activityLabels, by = 1)[, 2]
x <- rbind(xTest, xTrain)
colnames(x) <- features[, 2]
x <- cbind(subject, Activity, x)
# Mean and STD
search <- grep("-mean|-std", colnames(x))
data.mean.std <- x[,c(1,2,search)]
library(reshape2)
melted = melt(data.mean.std, id.var = c("Subject", "Activity"))
means = dcast(melted , subject + y ~ variable, mean)
means = dcast(melted , subject + y ~ variable, mean)
means = dcast(melted , subject + Activity ~ variable, mean)
means = dcast(melted , Subject + Activity ~ variable, mean)
?melt
run("run_analysis.R")
source("run_analysis.R")
?write.table
source("run_analysis.R")
write.table(tidyData, file = "./tidy_data.txt", row.names = FALSE)
#===============================================================================================
# Getting and Cleaning Data Class Assignment
#===============================================================================================
# The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set.
# The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers
# on a series of yes/no questions related to the project. You will be required to submit:
# 1) a tidy data set as described below,
# 2) a link to a Github repository with your script for performing the analysis, and
# 3) a code book that describes the variables, the data, and any transformations or work that you performed
# to clean up the data called CodeBook.md. You should also include a README.md in the repo with your scripts.
# This repo explains how all of the scripts work and how they are connected.
# One of the most exciting areas in all of data science right now is wearable computing - see for example
# this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced
# algorithms to attract new users. The data linked to from the course website represent data collected
# from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the
# site where the data was obtained:
# http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
# Here are the data for the project:
# https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
# You should create one R script called run_analysis.R that does the following.
#===============================================================================================
# Extracts only the measurements on the mean and standard deviation for each measurement.
# Uses descriptive activity names to name the activities in the data set
# Appropriately labels the data set with descriptive activity names.
## Read in the Ydata
ytest<-read.table("./UCI HAR Dataset/test/y_test.txt",header=F,col.names=c("ActivityID"))
ytr<-read.table("./UCI HAR Dataset/train/y_train.txt",header=F,col.names=c("ActivityID"))
## Read in the Subject ID Data
SubTest<-read.table("./UCI HAR Dataset/test/subject_test.txt",header=F,col.names=c("SubjectID"))
SubTr<-read.table("./UCI HAR Dataset/train/subject_train.txt",header=F,col.names=c("SubjectID"))
## Read in the File Features with Column Names
det<-read.table("./UCI HAR Dataset/features.txt", header=F, as.is=T, col.names=c("MeasureID", "MeasureName"))
## Read the X Data assigning column names from the features file
Xtest<-read.table("./UCI HAR Dataset/test/X_test.txt",header=F, col.names=det$MeasureName)
Xtr<-read.table("./UCI HAR Dataset/train/X_train.txt",header=F, col.names=det$MeasureName)
## Subset the column names
sub_det<- grep(".*mean\\(\\)|.*std\\(\\)", det$MeasureName)
## Subset the X data on the subset features
Xtest<-Xtest[,sub_det]
Xtr<-Xtr[,sub_det]
## Append the activity and Subject IDs
Xtest$ActivityID<-ytest$ActivityID
Xtest$SubjectID<-SubTest$SubjectID
Xtr$ActivityID<-ytr$ActivityID
Xtr$SubjectID<-SubTr$SubjectID
## Merge the update X files
data<-rbind(Xtest, Xtr)
cnames<-colnames(data)
cnames<-gsub("\\.+mean\\.+", cnames, replacement="Mean")
cnames<-gsub("\\.+std\\.+",  cnames, replacement="Std")
colnames(data)<-cnames
## Add an activiy names column
act<-read.table("/Users/Tyson/Desktop/DS3_Getting_and_Cleaning Data/UCI HAR Dataset/activity_labels.txt", header=F, as.is=T, col.names=c("ActivityID", "ActivityName"))
act$ActivityName<-as.factor(act$ActivityName)
lab_data<-merge(data,act)
#===============================================================================================
# Creates a second, independent tidy data set with the average of each variable for each
# activity and each subject.
library(reshape2)
## melt the dataset
id_vars=c("ActivityID", "ActivityName", "SubjectID")
measure_vars=setdiff(colnames(lab_data), id_vars)
melt_dat<-melt(lab_data, id=id_vars, measure.vars=measure_vars)
## recast
recas<-dcast(melted_data, ActivityName + SubjectID ~ variable, mean)
## Create the tidy data set and save it on to the named file
write.table(recas,"./UCI HAR Dataset/tidy_data.txt")
#===============================================================================================
setwd("~/GitHub/getting-and-cleaning-data-course-project/UCI HAR Dataset")
library(reshape2)
# Load the various datasets
test.subject <- read.table("./test/subject_test.txt")
test.x <- read.table("./test/X_test.txt")
test.y <- read.table("./test/y_test.txt")
train.subject <- read.table("./train/subject_train.txt")
train.x <- read.table("./train/X_train.txt")
train.y <- read.table("./train/y_train.txt")
features <- read.table("./features.txt")
activity.labels <- read.table("./activity_labels.txt")
# Merge the test and train subject datasets
subject <- rbind(test.subject, train.subject)
colnames(subject) <- "subject"
# Merge the test and train labels, applying the textual labels
label <- rbind(test.y, train.y)
label <- merge(label, activity.labels, by=1)[,2]
# Merge the test and train main dataset, applying the textual headings
data <- rbind(test.x, train.x)
colnames(data) <- features[, 2]
# Merge all three datasets
data <- cbind(subject, label, data)
# Create a smaller dataset containing only the mean and std variables
search <- grep("-mean|-std", colnames(data))
data.mean.std <- data[,c(1,2,search)]
# Compute the means, grouped by subject/label
melted = melt(data.mean.std, id.var = c("subject", "label"))
means = dcast(melted , subject + label ~ variable, mean)
# Save the resulting dataset
write.table(means, file="./data/tidy_data.txt")
# Output final dataset
means
write.table(means, file="./tidy_data.txt")
